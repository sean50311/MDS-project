{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329b2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2c5ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../../raw_data/'\n",
    "\n",
    "new_data = pd.read_csv(folder_path + 'AI4I-PMDI.csv')\n",
    "old_data = pd.read_csv(folder_path + 'ai4i2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d81a69",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f041b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 欄位名稱對應（僅要對應的欄位做改名）\n",
    "col_map = {\n",
    "    'Air temperature (K)': 'Air temperature',\n",
    "    'Process temperature (K)': 'Process temperature',\n",
    "    'Rotational speed (rpm)': 'Rotational speed',\n",
    "    'Torque (Nm)': 'Torque',\n",
    "    'Tool wear (min)': 'Tool wear',\n",
    "}\n",
    "# 用 rename 只處理指定欄位\n",
    "df = new_data.rename(columns=col_map).copy()\n",
    "\n",
    "# 2. 新增 failure 欄位\n",
    "for col in ['TWF', 'HDF', 'PWF', 'OSF', 'RNF']:\n",
    "    df[col] = 0\n",
    "\n",
    "# 3. 解析 Diagnostic 欄位，自動 one-hot\n",
    "failure_map = {\n",
    "    'Tool Wear Failure': 'TWF',\n",
    "    'Heat Dissipation Failure': 'HDF',\n",
    "    'Power Failure': 'PWF',\n",
    "    'Overstrain Failure': 'OSF',\n",
    "    'Random Failure': 'RNF'\n",
    "}\n",
    "\n",
    "def parse_failure(diag):\n",
    "    # 支援多個failure（分號、逗號、空格等分隔）\n",
    "    result = {k:0 for k in failure_map.values()}\n",
    "    if pd.isnull(diag) or diag.strip() == '' or diag == 'No Failure':\n",
    "        return result\n",
    "    # 允許一列有多個failure\n",
    "    for key, col in failure_map.items():\n",
    "        if key in diag:\n",
    "            result[col] = 1\n",
    "    return result\n",
    "\n",
    "failures = df['Diagnostic'].apply(parse_failure).apply(pd.Series)\n",
    "for col in ['TWF', 'HDF', 'PWF', 'OSF', 'RNF']:\n",
    "    df[col] = failures[col]\n",
    "    \n",
    "# 4. 新增 Machine failure 欄位（有任一 failure 則為 1）\n",
    "df['Machine failure'] = df[['TWF', 'HDF', 'PWF', 'OSF', 'RNF']].max(axis=1)\n",
    "\n",
    "processed_new_data = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "939f9ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共找到 12 個檔案\n",
      "../../instance/on_system\\ffill\\PMDI_imputed_ffill_knn_on_system.csv\n",
      "../../instance/on_system\\ffill\\PMDI_imputed_ffill_mean_on_system.csv\n",
      "../../instance/on_system\\ffill\\PMDI_imputed_ffill_median_on_system.csv\n",
      "../../instance/on_system\\ffill\\PMDI_imputed_ffill_rolling_on_system.csv\n",
      "../../instance/on_system\\linear\\PMDI_imputed_linear_knn_on_system.csv\n",
      "../../instance/on_system\\linear\\PMDI_imputed_linear_mean_on_system.csv\n",
      "../../instance/on_system\\linear\\PMDI_imputed_linear_median_on_system.csv\n",
      "../../instance/on_system\\linear\\PMDI_imputed_linear_rolling_on_system.csv\n",
      "../../instance/on_system\\rolling\\PMDI_imputed_rolling_knn_on_system.csv\n",
      "../../instance/on_system\\rolling\\PMDI_imputed_rolling_mean_on_system.csv\n",
      "../../instance/on_system\\rolling\\PMDI_imputed_rolling_median_on_system.csv\n",
      "../../instance/on_system\\rolling\\PMDI_imputed_rolling_rolling_on_system.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "read_file_path = '../../instance/on_system/'\n",
    "\n",
    "# 搜尋所有主資料夾下（例如 linear、ffill、rolling）子資料夾的所有 csv\n",
    "all_csv_paths = glob.glob(os.path.join(read_file_path, '*', '*.csv'))\n",
    "\n",
    "print(f'共找到 {len(all_csv_paths)} 個檔案')\n",
    "for path in all_csv_paths:\n",
    "    print(path)\n",
    "\n",
    "# 讀取全部表格\n",
    "dfs = []\n",
    "for path in all_csv_paths:\n",
    "    df = pd.read_csv(path)\n",
    "    dfs.append((os.path.basename(path), df))  # (檔名, df) 一起存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1810acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 設定你要補的欄位 ---\n",
    "cols = [\n",
    "    'Air temperature',\n",
    "    'Process temperature',\n",
    "    'Rotational speed',\n",
    "    'Torque',\n",
    "    'Tool wear'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc213258",
   "metadata": {},
   "source": [
    "# GAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2faf52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_out_dir = '../../instance/GAIN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5033e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在處理檔案：PMDI_imputed_ffill_knn_on_system.csv\n",
      "正在處理檔案：PMDI_imputed_ffill_mean_on_system.csv\n",
      "Epoch 10/50  |  D Loss: 0.0000  |  G Loss: 0.0032\n",
      "Epoch 20/50  |  D Loss: 0.0000  |  G Loss: 0.0014\n",
      "Epoch 30/50  |  D Loss: 0.0000  |  G Loss: 0.0005\n",
      "Epoch 40/50  |  D Loss: 0.0000  |  G Loss: 0.0006\n",
      "Epoch 50/50  |  D Loss: 0.0000  |  G Loss: 0.0004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gain_generator_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gain_generator_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_60 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │           \u001b[38;5;34m704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_61 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_62 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m5\u001b[0m)               │           \u001b[38;5;34m325\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,189</span> (20.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,189\u001b[0m (20.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,189</span> (20.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,189\u001b[0m (20.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型已完整儲存至：../../instance/GAIN\\gain_generator_model_with_toolwear.keras\n",
      "✅ 標準化器已儲存至：../../instance/GAIN\\gain_scaler_with_toolwear.pkl\n",
      "正在處理檔案：PMDI_imputed_ffill_median_on_system.csv\n",
      "正在處理檔案：PMDI_imputed_ffill_rolling_on_system.csv\n",
      "正在處理檔案：PMDI_imputed_linear_knn_on_system.csv\n",
      "正在處理檔案：PMDI_imputed_linear_mean_on_system.csv\n",
      "正在處理檔案：PMDI_imputed_linear_median_on_system.csv\n",
      "正在處理檔案：PMDI_imputed_linear_rolling_on_system.csv\n",
      "正在處理檔案：PMDI_imputed_rolling_knn_on_system.csv\n",
      "正在處理檔案：PMDI_imputed_rolling_mean_on_system.csv\n",
      "正在處理檔案：PMDI_imputed_rolling_median_on_system.csv\n",
      "正在處理檔案：PMDI_imputed_rolling_rolling_on_system.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 先假設你已經有：\n",
    "# - dfs: List of (檔案路徑, 讀進來的 DataFrame) \n",
    "# - cols: 要做補值的數值欄位列表，例如 ['Air temperature','Process temperature','Rotational speed', 'Torque','Tool wear']\n",
    "# - processed_new_data: 跟 df_scaled_orig 同形狀、維度的原始 DataFrame（包含 NaN），用於最終「決定要不要把 imputed 值套回去」\n",
    "# - root_out_dir: 輸出資料夾路徑\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# ----- GAIN 網路定義 -----\n",
    "class GainGenerator(keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(GainGenerator, self).__init__()\n",
    "        # Generator 的輸入：x_obs (缺失填 0 後) 與 mask\n",
    "        self.dense1 = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.dense2 = layers.Dense(hidden_dim, activation='relu')\n",
    "        # 最後重建整個維度\n",
    "        self.out_layer = layers.Dense(input_dim, activation=None)  # 無 activation，回傳實數\n",
    "\n",
    "    # def call(self, x_obs, m):\n",
    "    #     # x_obs, m 都是同一 batch shape (batch_size, D)\n",
    "    #     inp = tf.concat([x_obs, m], axis=1)\n",
    "    #     h = self.dense1(inp)\n",
    "    #     h = self.dense2(h)\n",
    "    #     x_tilde = self.out_layer(h)\n",
    "    #     return x_tilde\n",
    "\n",
    "    def call(self, inputs):  # inputs 是 shape=(batch, D*2) 的合併張量\n",
    "        h = self.dense1(inputs)\n",
    "        h = self.dense2(h)\n",
    "        x_tilde = self.out_layer(h)\n",
    "        return x_tilde\n",
    "\n",
    "# @register_keras_serializable()\n",
    "# class GainGenerator(keras.Model):\n",
    "#     def __init__(self, input_dim=8, hidden_dim=64, **kwargs):  # 給 default 值 + **kwargs 接 Keras 系統參數\n",
    "#         super(GainGenerator, self).__init__(**kwargs)\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.dense1 = layers.Dense(hidden_dim, activation='relu')\n",
    "#         self.dense2 = layers.Dense(hidden_dim, activation='relu')\n",
    "#         self.out_layer = layers.Dense(input_dim, activation=None)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         h = self.dense1(inputs)\n",
    "#         h = self.dense2(h)\n",
    "#         return self.out_layer(h)\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = super().get_config()\n",
    "#         config.update({\n",
    "#             'input_dim': self.input_dim,\n",
    "#             'hidden_dim': self.hidden_dim\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, config):\n",
    "#         return cls(**config)\n",
    "\n",
    "\n",
    "\n",
    "class GainDiscriminator(keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(GainDiscriminator, self).__init__()\n",
    "        # Discriminator 的輸入：x_hat (Generator 補值後的完整向量) 與 H_hint\n",
    "        self.dense1 = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.dense2 = layers.Dense(hidden_dim, activation='relu')\n",
    "        # 最後對每個維度都輸出一個 [0,1] 機率\n",
    "        self.out_layer = layers.Dense(input_dim, activation='sigmoid')\n",
    "\n",
    "    def call(self, x_hat, h):\n",
    "        inp = tf.concat([x_hat, h], axis=1)\n",
    "        h1 = self.dense1(inp)\n",
    "        h1 = self.dense2(h1)\n",
    "        d_prob = self.out_layer(h1)\n",
    "        return d_prob\n",
    "\n",
    "\n",
    "# 生成 Hint Vector 的簡單函式 (可調整 hint_rate)\n",
    "def sample_hint(m, hint_rate=0.9):\n",
    "    \"\"\"\n",
    "    m: mask 矩陣 (batch_size, D)，1 表示該位置原本可見，0 表示缺失\n",
    "    hint_rate: 保留原本 mask 的比例，其餘隨機設為 0（讓 Discriminator 無法得知全部缺失位置）\n",
    "    返回 shape=(batch_size, D) 的 Hint 矩陣\n",
    "    \"\"\"\n",
    "    # 隨機產生一個 [0,1] uniform，若 < hint_rate 就保留原本的 m，否則設定為 0\n",
    "    rand_uniform = tf.random.uniform(shape=tf.shape(m), minval=0., maxval=1.)\n",
    "    hint = tf.where(rand_uniform < hint_rate, m, tf.zeros_like(m))\n",
    "    return hint\n",
    "\n",
    "\n",
    "# 自訂損失：只對可見位置 (m=1) 做 Reconstruct Loss，對缺失位置 (m=0) 做 Adversarial Loss\n",
    "mse_loss = keras.losses.MeanSquaredError()\n",
    "bce_loss = keras.losses.BinaryCrossentropy(from_logits=False)  # 因為 Disc 最後一層有 sigmoid\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 主要流程：跑 GAIN 補值\n",
    "# ---------------------------------------------------\n",
    "for path, df in dfs:\n",
    "    print(f\"正在處理檔案：{path}\")\n",
    "    if \"PMDI_imputed_ffill_mean\" not in path:\n",
    "        continue\n",
    "\n",
    "    # 1. 對要補值的 cols 做標準化\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(df[cols]),\n",
    "        columns=cols\n",
    "    )\n",
    "\n",
    "    # 2. 準備訓練資料：X_obs, M_mask\n",
    "    data_matrix = df_scaled.values.astype(np.float32)  # shape = (N, D)\n",
    "    m_mask = (~np.isnan(data_matrix)).astype(np.float32)  # 原始非缺失處為 1，缺失處為 0\n",
    "\n",
    "    # 如果有 NaN 先用 0 填滿，方便 feed 進網路\n",
    "    X_obs = np.nan_to_num(data_matrix, nan=0.0).astype(np.float32)\n",
    "\n",
    "    N, D = X_obs.shape\n",
    "\n",
    "    # 3. 建立 Generator、Discriminator 物件\n",
    "    generator = GainGenerator(input_dim=D, hidden_dim=64)\n",
    "    discriminator = GainDiscriminator(input_dim=D, hidden_dim=64)\n",
    "\n",
    "    # 4. 定義 Optimizer\n",
    "    g_optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    d_optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "    # 5. 訓練參數\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS = 50\n",
    "    HINT_RATE = 0.9\n",
    "    ALPHA = 10.0  # Reconstruction Loss 的權重 (可自行調整)\n",
    "\n",
    "    # 6. 轉成 tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_obs, m_mask))\n",
    "    dataset = dataset.shuffle(buffer_size=N).batch(BATCH_SIZE)\n",
    "\n",
    "    # 7. 訓練迴圈\n",
    "    for epoch in range(EPOCHS):\n",
    "        for step, (x_batch, m_batch) in enumerate(dataset):\n",
    "            # 7.1 產生 hint\n",
    "            h_batch = sample_hint(m_batch, hint_rate=HINT_RATE)\n",
    "\n",
    "            # 7.2 Generator forward & compute Generator Loss\n",
    "            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "                # Generator 補值：x_tilde = G(X_obs, M_mask)\n",
    "                generator_input = tf.concat([x_batch, m_batch], axis=1)\n",
    "                x_tilde = generator(generator_input)\n",
    "                # 用 x_hat 表示「補值後的完整向量」：x_hat = M * X_obs + (1-M) * x_tilde\n",
    "                x_hat = m_batch * x_batch + (1.0 - m_batch) * x_tilde\n",
    "\n",
    "                # Discriminator forward on「補值後」與 hint\n",
    "                d_prob = discriminator(x_hat, h_batch)\n",
    "\n",
    "                # Discriminator Loss = BCE( M, D(x_hat, hint) ) (只在 hint=1 的位置計)\n",
    "                d_loss = bce_loss(m_batch * h_batch, d_prob * h_batch)\n",
    "\n",
    "                # Generator 的 Adversarial loss: 希望 D 在缺失位置 (1-M) 預測為 1\n",
    "                g_adv_loss = bce_loss((1.0 - m_batch), d_prob * (1.0 - m_batch))\n",
    "\n",
    "                # Generator 的 Reconstruction loss: 只在原本可見位置 (M=1) 計算 MSE(x_obs, x_tilde)\n",
    "                g_rec_loss = mse_loss(x_batch * m_batch, x_tilde * m_batch)\n",
    "\n",
    "                # 總 Generator Loss = Adv + α * Rec\n",
    "                g_loss = g_adv_loss + ALPHA * g_rec_loss\n",
    "\n",
    "            # 7.3 計算 Gradient 並更新參數\n",
    "            gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "            gradients_of_discriminator = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "            g_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "            d_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "        # （可選）每隔若干 epoch 印一次 Loss\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}  |  D Loss: {d_loss.numpy():.4f}  |  G Loss: {g_loss.numpy():.4f}\")\n",
    "\n",
    "    # 訓練完後儲存整個模型（注意：需要合併 x_obs 與 mask）\n",
    "    generator_input_shape = (None, D * 2)\n",
    "    generator.build(input_shape=generator_input_shape)\n",
    "    generator.summary()  # 確保模型真的被建構\n",
    "\n",
    "    model_path = os.path.join(root_out_dir, 'gain_generator_model_with_toolwear.keras')\n",
    "    generator.save(model_path)\n",
    "    print(f\"✅ 模型已完整儲存至：{model_path}\")\n",
    "\n",
    "    # gen_weights_path = os.path.join(root_out_dir, 'gain_generator.weights.h5')\n",
    "    # generator.save_weights(gen_weights_path)\n",
    "    # print(f\"✅ Generator 權重已儲存至：{gen_weights_path}\")\n",
    "\n",
    "    scaler_path = os.path.join(root_out_dir, 'gain_scaler_with_toolwear.pkl')\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"✅ 標準化器已儲存至：{scaler_path}\")\n",
    "    continue\n",
    "\n",
    "    # 8. 訓練完成後，用整張表一次補值\n",
    "    X_obs_tensor = tf.convert_to_tensor(X_obs, dtype=tf.float32)\n",
    "    M_mask_tensor = tf.convert_to_tensor(m_mask, dtype=tf.float32)\n",
    "    H_hint_tensor = sample_hint(M_mask_tensor, hint_rate=HINT_RATE)\n",
    "\n",
    "    # Generator 補值\n",
    "    x_tilde_full = generator(X_obs_tensor, M_mask_tensor)  # (N, D)\n",
    "    x_hat_full = M_mask_tensor * X_obs_tensor + (1.0 - M_mask_tensor) * x_tilde_full\n",
    "    x_hat_full_np = x_hat_full.numpy()  # np array\n",
    "\n",
    "    # 9. 建構補值後的 DataFrame（還是標準化後）\n",
    "    df_imputed_scaled = pd.DataFrame(x_hat_full_np, columns=cols)\n",
    "\n",
    "    # 10. 反標準化\n",
    "    df_imputed = pd.DataFrame(\n",
    "        scaler.inverse_transform(df_imputed_scaled[cols]),\n",
    "        columns=cols\n",
    "    )\n",
    "\n",
    "    # 11. 最後把補值貼回原本 processed_new_data，只覆寫那些 NaN 的 cell\n",
    "    df_result = processed_new_data.copy()\n",
    "    for col in cols:\n",
    "        is_na = processed_new_data[col].isna()\n",
    "        df_result.loc[is_na, col] = df_imputed.loc[is_na, col]\n",
    "\n",
    "    # 12. 把最終結果存檔\n",
    "    # file_name = os.path.basename(path)\n",
    "    # new_filename = file_name.replace('.csv', '_GAIN.csv')\n",
    "    # out_path = os.path.join(root_out_dir, new_filename)\n",
    "    # df_result.to_csv(out_path, index=False)\n",
    "    # print(f\"已儲存至：{out_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7a72f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gain_generator_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gain_generator_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │           \u001b[38;5;34m576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m4\u001b[0m)               │           \u001b[38;5;34m260\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,996</span> (19.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,996\u001b[0m (19.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,996</span> (19.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,996\u001b[0m (19.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde07bed",
   "metadata": {},
   "source": [
    "## auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6612c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "099fad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_out_dir = '../../instance/auto-encoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例如你要對每張表跑 autoencoder\n",
    "for path, df in dfs:\n",
    "    print(f\"正在處理：{path}\")\n",
    "    # 2. 標準化\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(df[cols]),\n",
    "        columns=cols\n",
    "    )\n",
    "\n",
    "    train_data = df_scaled.values  # 補值過的完整資料\n",
    "\n",
    "    # 3. 建立 Autoencoder\n",
    "    input_dim = train_data.shape[1]\n",
    "    input_layer = keras.Input(shape=(input_dim,))\n",
    "    encoded = layers.Dense(8, activation='relu')(input_layer)\n",
    "    encoded = layers.Dense(4, activation='relu')(encoded)\n",
    "    decoded = layers.Dense(8, activation='relu')(encoded)\n",
    "    decoded = layers.Dense(input_dim)(decoded)\n",
    "\n",
    "    autoencoder = keras.Model(inputs=input_layer, outputs=decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # 4. 訓練 autoencoder\n",
    "    autoencoder.fit(train_data, train_data, epochs=100, batch_size=64, verbose=1)\n",
    "\n",
    "    # 5. 用 autoencoder 來補原始有缺值的資料\n",
    "    def ae_impute_row(row, model, scaler):\n",
    "        row = row.values.astype(float)\n",
    "        nan_idx = np.isnan(row)\n",
    "        if not np.any(nan_idx):\n",
    "            return row  # 沒缺值直接回傳\n",
    "        row_filled = row.copy()\n",
    "        row_filled[nan_idx] = 0  # 先補0\n",
    "        row_filled = row_filled.reshape(1, -1)\n",
    "        pred = model.predict(row_filled, verbose=0)[0]\n",
    "        # 只補 nan 欄位\n",
    "        row[nan_idx] = pred[nan_idx]\n",
    "        return row\n",
    "\n",
    "    df_scaled_orig = pd.DataFrame(\n",
    "        scaler.transform(processed_new_data[cols]),\n",
    "        columns=cols\n",
    "    )\n",
    "\n",
    "    df_imputed_scaled = df_scaled_orig.copy()\n",
    "    for i, row in df_scaled_orig.iterrows():\n",
    "        if row.isna().any():\n",
    "            df_imputed_scaled.iloc[i] = ae_impute_row(row, autoencoder, scaler)\n",
    "\n",
    "    # 6. 反標準化\n",
    "    df_imputed = pd.DataFrame(\n",
    "        scaler.inverse_transform(df_imputed_scaled),\n",
    "        columns=cols\n",
    "    )\n",
    "\n",
    "    # 7. 補回原本資料\n",
    "    df_result = processed_new_data.copy()\n",
    "    for col in cols:\n",
    "        df_result[col] = np.where(processed_new_data[col].isna(), df_imputed[col], processed_new_data[col])\n",
    "\n",
    "    df_result['Tool wear'] = df['Tool wear']\n",
    "\n",
    "\n",
    "    # 分類儲存路徑\n",
    "    # category = os.path.basename(os.path.dirname(path))\n",
    "    file_name = os.path.basename(path)\n",
    "    new_filename = file_name.replace('.csv', '_autoencoder.csv')\n",
    "    # out_dir = os.path.join(root_out_dir, category)\n",
    "    # os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(root_out_dir, new_filename)\n",
    "    df_result.to_csv(out_path, index=False)\n",
    "    print(f'儲存 {out_path}')\n",
    "\n",
    "    # # 4. 儲存\n",
    "    # df_result.to_csv(out_path, index=False)\n",
    "    # print(f'儲存 {out_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
